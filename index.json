[{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/tags/apisix/","section":"Tags","summary":"","title":"Apisix","type":"tags"},{"content":" Overview: # This project involved implementing APISIX, a high-performance API gateway, to enhance API management for our advanced analytics team. The goal was to gain control over our API usage, which was previously consumed by other teams without proper oversight, and enable easy routing to our PostgreSQL database.x\nKey Features: # API Usage Visibility: Before APISIX, our team lacked a clear overview of how our APIs were being consumed. With APISIX, we can now monitor and manage API traffic more effectively. Database Integration: We were unable to create routes to our PostgreSQL database, limiting our flexibility. With APISIX, routing to databases and other services is seamless. Effortless API Management: APISIX has simplified API management, making it not only more functional but also enjoyable to work with. Business Value: # Improved Team Collaboration: Our analytics team now has complete oversight of how our APIs are used by other teams, ensuring better resource allocation. Streamlined API Workflow: Easier management of API routes has led to more efficient workflows and faster response times for database queries. Scalability and Flexibility: APISIX provides a scalable solution that allows for easy adaptation as our API needs evolve. Explore the integration details and codebase on GitHub. GitHub Repository\n","date":"20 June 2024","externalUrl":null,"permalink":"/projects/apisix/","section":"Projects","summary":"","title":"APISIX API Gateway - Streamlining API Management","type":"projects"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/tags/app/","section":"Tags","summary":"","title":"App","type":"tags"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/","section":"Berry Blom","summary":"","title":"Berry Blom","type":"page"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/tags/consulting/","section":"Tags","summary":"","title":"Consulting","type":"tags"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/tags/openai/","section":"Tags","summary":"","title":"Openai","type":"tags"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"20 June 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Overview: # MailReplai is an Outlook add-on designed to streamline email communication by leveraging AI to generate smart, context-aware responses directly within your inbox. No more switching between your email client and external tools‚ÄîMailReplai brings the power of AI right into your workflow.\nKey Features: # AI-Driven Email Responses: Uses GPT-3 to generate high-quality responses to emails based on context, saving time and improving communication efficiency. Seamless Integration: Fully integrated into Outlook, allowing users to stay focused on their inbox without needing to jump between platforms. Customizable Replies: Tailor the tone and style of responses to suit professional or casual settings. Business Value: # Increased Productivity: Automates repetitive email tasks, freeing up valuable time for more strategic work. Consistent Communication: Ensures that email responses are consistently professional and aligned with your desired tone. Reduced Response Time: Allows users to handle emails more efficiently, improving response time and overall communication flow. Check out the code and try MailReplai on GitHub. GitHub Repository\n","date":"18 April 2024","externalUrl":null,"permalink":"/projects/mailreplai/","section":"Projects","summary":"","title":"MailReplai - AI-Powered Email Response Assistant","type":"projects"},{"content":"","date":"18 April 2024","externalUrl":null,"permalink":"/tags/outlook-add-in/","section":"Tags","summary":"","title":"Outlook Add In","type":"tags"},{"content":"","date":"18 April 2024","externalUrl":null,"permalink":"/tags/startup/","section":"Tags","summary":"","title":"Startup","type":"tags"},{"content":"Hello! I\u0026rsquo;m Berry, an AI Engineer with over a decade of experience in Artificial Intelligence and Data Science. My passion lies in leveraging AI to help businesses become more efficient and competitive.\nWhat I Do # In my professional journey, I\u0026rsquo;ve had the opportunity to work on a variety of projects across diverse industries. Here are some of the key areas I specialize in:\nü§ñ AI and Automation Consulting: Helping businesses understand and integrate AI and automation solutions. üéì AI Training: Providing training sessions to help teams develop AI skills. üõ† AI Product Development: Building custom AI products from concept to deployment. üó£ Coaching: Offering one-on-one coaching to help professionals grow their AI expertise. Current Focus # My current focus is on scaling AI training programs to help more businesses become efficient using AI. By providing comprehensive training sessions and hands-on workshops, I aim to empower organizations to leverage AI technologies effectively.\nI am also working on a top-secret project for the Dutch government, pushing the boundaries of AI innovation. üé¨üïµÔ∏è‚Äç‚ôÇÔ∏è\nAcademic Journey # I hold a master\u0026rsquo;s degree in Data Science from the University of Amsterdam, where I conducted significant research in the field of AI. My research has been published on various platforms. One notable project was on domain adaptation in transformer models for question answering on Dutch government policies.\nDomain Adaptation in Transformer models: Question Answering of Dutch Government Policies üìö Previous Experience # I\u0026rsquo;ve collaborated with a range of clients, delivering impactful AI and data solutions that drive business success:\nDutch Government Worked on setting up local LLMs and a scheduling mechanism for other teams to efficiently utilize our servers.\nThis improved resource allocation and increased productivity across multiple departments.\nTechnologies used: Python, Docker, Kubernetes, and custom scheduling algorithms.\nKPN Created parsing logic in Python and Scala, and enhanced the monitoring system for Dutch railways using Prometheus and Grafana.\nThis resulted in improved operational efficiency and real-time insights into railway operations.\nTechnologies used: Python, Scala, Prometheus, Grafana, and big data processing tools.\nVodafoneZiggo Assisted in migrating data pipelines, ensuring a smooth transition and reliable data flow.\nThis modernization effort improved data processing speeds and reliability of analytics.\nTechnologies used: Apache Spark, Hadoop, and cloud migration tools.\nMileway Developed a document management system to automatically detect real estate documents, streamlining document handling and increasing accuracy.\nThis system significantly reduced manual processing time and improved data extraction accuracy.\nTechnologies used: Machine Learning, OCR, Python, and document processing libraries.\nRabobank Helped develop the data marketplace and set up data infrastructure, enabling better data management and accessibility.\nThis resulted in improved operational efficiency and real-time insights into railway operations.\nTechnologies used: Python, Pulummi, Spark, Azure pipelines.\nAhold Created dashboards that provide insightful data visualizations, aiding in informed decision-making.\nThis modernization effort improved data oversight of analytics.\nTechnologies used: Power Bi.\nUnicef Built an Azure data platform from scratch to enhance marketing efforts, significantly improving campaign effectiveness.\nThis platform increased markering effort and allows for microcampaigns resulting in a doubling of donations.\nTechnologies used: Azure, Dynamics, Python, and document processing libraries.\nEach project highlights my commitment to delivering tailored AI and data solutions that provide tangible benefits to my clients.\nLet\u0026rsquo;s Connect # I‚Äôm always excited to connect with like-minded professionals and explore new opportunities. Whether you\u0026rsquo;re looking to implement AI solutions in your business or want to enhance your team\u0026rsquo;s AI capabilities, I\u0026rsquo;d love to help.\nYou can connect with me on the following platforms:\nüê¶ X üíº LinkedIn üíª GitHub ","date":"9 April 2024","externalUrl":null,"permalink":"/about/","section":"Berry Blom","summary":"","title":"About me","type":"page"},{"content":" Intro # During my Master\u0026rsquo;s in Data Science at the University of Amsterdam, I worked on creating a Question Answering (QA) system for Dutch government policies using transformer models. This research aimed to improve how these models handle specific domains like government policies. In this blog post, I\u0026rsquo;ll share my approach, the new dataset we developed, and the key findings that enhanced the performance of our QA system.\nAcademic presentation # In November 2023, I presented my research at the University of Evora in Portugal. The audience consisted of academic researchers specializing in various AI fields, including computer vision, optimization, and other areas. I was part of the natural language processing group. Following this presentation, the research was published in collaboration with my UVA supervisor.\nI met many cool people during this event, with whom I am still in contact. This experience not only helped me share my work but also allowed me to network with other AI professionals.\nEvora, Portugal - November 24 2023\nAbstract # Automatic answering questions helps users in finding information efficiently, in contrast with web search engines that require keywords to be provided and large texts to be processed. The first Dutch Question Answering (QA) system uses basic natural language processing techniques based on text similarity between the question and the answer. After the introduction of pre-trained transformer-based models like BERT, higher scores were achieved with over 7.7% improvement for the General Language Understanding Evaluation (GLUE) score.\nPre-trained transformer-based models tend to over-generalize when applied to a specific domain, leading to less precise context-specific outputs. There is a marked research gap in experiment strategies to adapt these models effectively for domain-specific applications. Additionally, there is a lack of Dutch resources for automatic question answering, as the only existing dataset, Dutch SQuAD, is a translation of the SQuAD dataset in English.\nWe propose a new dataset, PolicyQA, containing questions and answers about Dutch government policies and use domain adaptation techniques to address the generalizability problem of transformer-based models.\nThe experimental setup includes the Long Short-Term memory (LSTM), a baseline neural network, and three BERT-based models, mBert, RobBERT, and BERTje, with domain adaptation. The datasets used for testing are the proposed PolicyQA dataset and the existing Dutch SQuAD.\nFrom the results, we found that the multilanguage BERT-model, mBert, outperforms the Dutch BERT-based models (RobBERT and BERTje) on the both datasets. By introducing fine-tuning, a domain adaptation technique, the mBert model improved to 94.10% of F1-score, a gain of 226% compared to its performance without fine-tuning.\nFull-text article # Read article\n","date":"4 August 2022","externalUrl":null,"permalink":"/projects/domainadaptationintransformers/","section":"Projects","summary":"","title":"Domain Adaptation in Transformer models: Question Answering of Dutch Government Policies","type":"projects"},{"content":"","date":"4 August 2022","externalUrl":null,"permalink":"/tags/first-post/","section":"Tags","summary":"","title":"First Post","type":"tags"},{"content":"","date":"4 August 2022","externalUrl":null,"permalink":"/tags/hello-world/","section":"Tags","summary":"","title":"Hello World","type":"tags"},{"content":" Preview # The essential Streamlit for all your data science needs # To build a web app you‚Äôd typically use such Python web frameworks as Django and Flask. But the steep learning curve and the big time investment for implementing these apps present a major hurdle.\nStreamlit makes the app creation process as simple as writing Python scripts!\nIn this article, you‚Äôll learn how to master Streamlit when getting started with data science.\nLet‚Äôs dive in!\nRead blog\n","date":"8 April 2022","externalUrl":null,"permalink":"/posts/how-to-master-streamlit-for-data-science/","section":"Posts","summary":"","title":"How to master Streamlit for Data Science","type":"posts"},{"content":"","date":"8 April 2022","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"15 February 2022","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":" Enhancing Common Sense in Large Language Models: Insights from a Kaggle Competition # Endowing AI with common sense, the intuitive understanding of everyday situations, remains a significant hurdle. This challenge is crucial as it underpins the natural language understanding capabilities of AI, making interactions more human-like and reliable. Recently, I participated in a Kaggle competition aimed at evaluating and improving the common sense reasoning of natural language understanding systems. Here‚Äôs an overview of the competition, its importance, and my experience.\nThe Importance of Common Sense in AI # Introducing common sense to natural language understanding systems has garnered increasing attention within the research community. Unlike specific, domain-based knowledge, common sense involves a broad understanding of the world, enabling systems to handle ambiguous and context-dependent scenarios effectively. The ability to identify why a statement does not make sense is fundamental to building robust AI models capable of interacting with humans in a more intuitive and meaningful way.\nThe Challenge: Common Sense Reasoning # The competition posed a unique and intriguing task: Given a statement that defies common sense, can a model identify the best explanation for why it does not make sense? Participants were required to choose the most appropriate reason from three given options. For example:\nStatement: He put an elephant into the fridge.\nA: An elephant is much bigger than a fridge. (correct) B: Elephants are usually white while fridges are usually white. C: An elephant cannot eat a fridge. In this instance, option A is correct because it directly addresses the implausibility of the statement based on the relative sizes of an elephant and a fridge.\nEvaluation and Submission # The evaluation was straightforward yet challenging. Submissions were assessed based on category accuracy, which measures the proportion of correctly identified explanations:\n[ \\text{Category Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i = \\hat{y}_i) ]\nWhere ( N ) is the number of sentences in the test set, ( y_i ) is the true label, and ( \\hat{y}_i ) is the predicted label. Achieving a score around 0.33, akin to random guessing, was the baseline.\nParticipants were required to submit their predictions in a CSV format, containing the sentence IDs and the corresponding predicted labels. The dataset comprised 8000 sentences for training and 2000 sentences for testing.\nDataset and Files # The provided dataset included:\ntrain_data.csv: The training set with sentences and options. train_answers.csv: Correct labels for the training set. test_data.csv: The test set. sample.csv: A sample submission file in the correct format. Each data entry contained:\nid: An identifier for each sentence. FalseSent: The nonsensical sentence. OptionA, OptionB, OptionC: Potential explanations. answer: The correct answer among the options. My Experience and Approach # Participating in this competition was both a challenging and enlightening experience. I utilized several techniques to enhance the model\u0026rsquo;s ability to discern common sense. Here‚Äôs a brief overview of my approach:\nData Preprocessing: Cleaning and preparing the dataset for efficient model training. Model Selection: Leveraging transformer-based models such as BERT, known for their superior natural language understanding capabilities. Fine-tuning: Adapting pre-trained models to the specific task of common sense reasoning using the provided dataset. Evaluation and Optimization: Iteratively evaluating model performance on a validation set and optimizing hyperparameters to improve accuracy. Conclusion # The competition highlighted the ongoing challenges and advancements in integrating common sense into AI systems. By tackling seemingly simple yet fundamentally complex tasks, we push the boundaries of what AI can achieve, moving closer to creating systems that understand and interact with the world more naturally.\nFor more details about the competition, you can visit the Kaggle competition page.\n","date":"15 February 2022","externalUrl":null,"permalink":"/posts/common-sense-in-llm/","section":"Posts","summary":"","title":"Enhancing Common Sense in Large Language Models: Insights from a Kaggle Competition","type":"posts"},{"content":"","date":"15 February 2022","externalUrl":null,"permalink":"/tags/llms/","section":"Tags","summary":"","title":"LLMs","type":"tags"},{"content":" Overview: # V.I.C.T.O.R.Y. is an AI-powered application designed to identify non-inclusive language in job vacancies, particularly targeting age, gender, and background biases. This project aims to address youth unemployment by ensuring that job postings are inclusive and accessible.\nKey Features: # Discrimination Detection: Uses Natural Language Processing (NLP) to identify potentially discriminatory language in job vacancy texts. Machine Learning: Trained a Multinomial Naive Bayes model for binary classification of vacancies as inclusive or non-inclusive. Real-time Feedback: Highlights non-inclusive words in real-time and provides suggestions for improving vacancy text. Technologies: # Streamlit: Built a user-friendly web interface for job vacancy analysis and feedback visualization. NLTK (Natural Language Toolkit): Applied for text preprocessing and pattern matching of discriminatory language. GPT-3: Integrated for generating inclusive word suggestions to enhance job postings. Explore the codebase and try out the project on GitHub. GitHub Repository\n","date":"7 February 2022","externalUrl":null,"permalink":"/projects/vacancy-inclusivity-checker/","section":"Projects","summary":"","title":"Project: V.I.C.T.O.R.Y - Vacancy Inclusivity Checker","type":"projects"},{"content":"","date":"7 February 2022","externalUrl":null,"permalink":"/tags/streamlit/","section":"Tags","summary":"","title":"Streamlit","type":"tags"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]